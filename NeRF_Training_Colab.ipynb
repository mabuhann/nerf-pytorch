{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeRF Training on Google Colab with Automated Metrics\n",
    "\n",
    "This notebook trains NeRF on the LEGO dataset and automatically tracks all metrics.\n",
    "\n",
    "**Expected Runtime**: 4-6 hours on Colab GPU (T4)\n",
    "\n",
    "**What you'll get**:\n",
    "- Training time\n",
    "- Test PSNR & SSIM\n",
    "- All metrics saved automatically\n",
    "- Downloadable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install imageio imageio-ffmpeg configargparse scikit-image tqdm matplotlib -q\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive\n",
    "\n",
    "**Important**: Upload all your files to Google Drive first:\n",
    "1. Create a folder: `My Drive/nerf_training/`\n",
    "2. Upload your downloaded files:\n",
    "   - `run_nerf_with_metrics.py`\n",
    "   - `run_nerf_helpers.py`\n",
    "   - `load_blender.py`\n",
    "   - `load_llff.py`\n",
    "   - `load_deepvoxels.py`\n",
    "   - `load_LINEMOD.py`\n",
    "   - `lego_config.txt`\n",
    "   - `analyze_results.py`\n",
    "3. Upload your dataset folder: `data/` (with the lego dataset inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✓ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to your files in Google Drive\n",
    "# CHANGE THIS to match your Google Drive folder structure\n",
    "DRIVE_PATH = '/content/drive/MyDrive/nerf_training'\n",
    "\n",
    "# Change to that directory\n",
    "os.chdir(DRIVE_PATH)\n",
    "\n",
    "# Verify files exist\n",
    "print(\"Checking files...\\n\")\n",
    "\n",
    "required_files = [\n",
    "    'run_nerf_with_metrics.py',\n",
    "    'run_nerf_helpers.py',\n",
    "    'load_blender.py',\n",
    "    'lego_config.txt'\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"✓ {f}\")\n",
    "    else:\n",
    "        print(f\"✗ {f} NOT FOUND\")\n",
    "        all_good = False\n",
    "\n",
    "# Check data\n",
    "data_path = 'data/nerf_synthetic/lego/transforms_train.json'\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"✓ Dataset found at {data_path}\")\n",
    "else:\n",
    "    print(f\"✗ Dataset NOT FOUND at {data_path}\")\n",
    "    all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n✓ All files found! Ready to train.\")\n",
    "else:\n",
    "    print(\"\\n✗ Some files missing. Please upload them to Google Drive.\")\n",
    "\n",
    "# Show current directory\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")\n",
    "print(f\"\\nFiles in directory:\")\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Necessary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configs directory if it doesn't exist\n",
    "!mkdir -p configs\n",
    "\n",
    "# Move config file to configs directory if needed\n",
    "if os.path.exists('lego_config.txt') and not os.path.exists('configs/lego_config.txt'):\n",
    "    !mv lego_config.txt configs/\n",
    "    print(\"✓ Moved lego_config.txt to configs/\")\n",
    "\n",
    "# Create logs directory\n",
    "!mkdir -p logs\n",
    "\n",
    "print(\"✓ Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure check:\\n\")\n",
    "\n",
    "base_path = 'data/nerf_synthetic/lego'\n",
    "\n",
    "# Check transforms files\n",
    "for split in ['train', 'val', 'test']:\n",
    "    transform_file = f'{base_path}/transforms_{split}.json'\n",
    "    if os.path.exists(transform_file):\n",
    "        with open(transform_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        n_frames = len(data['frames'])\n",
    "        print(f\"✓ {split}: {n_frames} images\")\n",
    "    else:\n",
    "        print(f\"✗ {transform_file} not found\")\n",
    "\n",
    "# Count actual image files\n",
    "for split in ['train', 'val', 'test']:\n",
    "    img_dir = f'{base_path}/{split}'\n",
    "    if os.path.exists(img_dir):\n",
    "        n_images = len([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "        print(f\"  → {split} images on disk: {n_images}\")\n",
    "    else:\n",
    "        print(f\"  → {img_dir} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Preview Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the configuration\n",
    "print(\"Configuration for training:\\n\")\n",
    "!cat configs/lego_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Start Training\n",
    "\n",
    "**⚠️ IMPORTANT**: This will take 4-6 hours. Make sure:\n",
    "1. You're using a GPU runtime (Runtime → Change runtime type → GPU)\n",
    "2. Your Colab session won't time out (keep the tab open or use Colab Pro)\n",
    "3. You have enough Google Drive storage for checkpoints (~5GB)\n",
    "\n",
    "The training will:\n",
    "- Run for 200,000 iterations\n",
    "- Save checkpoints every 10,000 iterations\n",
    "- Evaluate on test set every 25,000 iterations\n",
    "- Print progress every 100 iterations\n",
    "- Save all metrics automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python run_nerf_with_metrics.py --config configs/lego_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "!python analyze_results.py \\\n",
    "    --log_dir logs/lego_metrics \\\n",
    "    --save_plot logs/lego_metrics/detailed_analysis.png \\\n",
    "    --export_table logs/lego_metrics/results_table.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "# Load summary metrics\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('logs/lego_metrics/summary_metrics.json', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display training plot\n",
    "print(\"\\nTraining Metrics Plot:\")\n",
    "display(Image('logs/lego_metrics/training_metrics.png'))\n",
    "\n",
    "# Display detailed analysis\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "display(Image('logs/lego_metrics/detailed_analysis.png'))\n",
    "\n",
    "# Display markdown table\n",
    "print(\"\\nResults Table (copy this to your report):\")\n",
    "with open('logs/lego_metrics/results_table.md', 'r') as f:\n",
    "    table_content = f.read()\n",
    "display(Markdown(table_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Download Results\n",
    "\n",
    "You can download specific files or zip everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all results\n",
    "!cd logs && zip -r lego_metrics_results.zip lego_metrics/\n",
    "\n",
    "print(\"✓ Results zipped to: logs/lego_metrics_results.zip\")\n",
    "print(\"\\nYou can find this file in your Google Drive at:\")\n",
    "print(f\"{DRIVE_PATH}/logs/lego_metrics_results.zip\")\n",
    "\n",
    "# List key files\n",
    "print(\"\\nKey result files:\")\n",
    "!ls -lh logs/lego_metrics/*.json\n",
    "!ls -lh logs/lego_metrics/*.npz\n",
    "!ls -lh logs/lego_metrics/*.png\n",
    "!ls -lh logs/lego_metrics/*.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Quick Metrics Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick extraction of key metrics for your report\n",
    "import json\n",
    "\n",
    "with open('logs/lego_metrics/summary_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KEY METRICS FOR YOUR REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Time: {metrics['total_training_time_hours']:.2f} hours\")\n",
    "print(f\"Test PSNR: {metrics['best_test_psnr']:.2f} dB\")\n",
    "print(f\"Test SSIM: {metrics['avg_test_ssim']:.4f}\")\n",
    "print(f\"Avg Step Time: {metrics['avg_step_time']:.4f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nCopy these values for comparison with 3DGS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Render Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render test set images (if you want to visualize quality)\n",
    "!python run_nerf_with_metrics.py \\\n",
    "    --config configs/lego_config.txt \\\n",
    "    --render_only \\\n",
    "    --render_test\n",
    "\n",
    "print(\"\\nRendered images saved to: logs/lego_metrics/renderonly_test_*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory Error\n",
    "Edit `configs/lego_config.txt` and reduce:\n",
    "```\n",
    "N_rand = 512      # Reduce from 1024\n",
    "chunk = 4096      # Reduce from 8192\n",
    "netchunk = 65536  # Reduce from 131072\n",
    "```\n",
    "\n",
    "### Session Timeout\n",
    "- Use Colab Pro for longer sessions\n",
    "- Or train for fewer iterations (change N_iters in the code to 100000)\n",
    "\n",
    "### Files Not Found\n",
    "- Make sure all files are uploaded to Google Drive\n",
    "- Check the DRIVE_PATH variable in Step 4\n",
    "\n",
    "### Slow Training\n",
    "- Make sure you're using GPU runtime\n",
    "- Check GPU type: T4 is standard, A100 is fastest\n",
    "- Consider using half_res = True in config for faster training"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
